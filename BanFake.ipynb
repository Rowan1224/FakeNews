{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as python_random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.python.keras.losses import BinaryCrossentropy\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tqdm.keras import TqdmCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom F1 score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred): \n",
    "\n",
    "    \"\"\"Return F1 score of Fake class\"\"\"\n",
    "    \n",
    "    \n",
    "    def recall_m(y_true, y_pred):\n",
    "\n",
    "        \"\"\"Return recall score of Fake class\"\"\"\n",
    "\n",
    "        #altering the labels to calculate the recall \n",
    "        y_true = K.round(K.clip(y_true, 0, 1))\n",
    "        y_true = K.round(K.clip((y_true-1)*-1, 0, 1))\n",
    "\n",
    "        y_pred= K.round(K.clip(y_pred, 0, 1))\n",
    "        y_pred = K.round(K.clip((y_pred-1)*-1, 0, 1))\n",
    "\n",
    "        #count the number of correct Fake prediction\n",
    "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "\n",
    "        #count number of true Fake entries\n",
    "        Fakes = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        \n",
    "        recall = TP / (Fakes+K.epsilon())    \n",
    "        return recall \n",
    "    \n",
    "    \n",
    "    def precision_m(y_true, y_pred):\n",
    "\n",
    "        \"\"\"Return precision score of Fake class\"\"\"\n",
    "\n",
    "        #altering the labels to calculate the precision \n",
    "        y_true = K.round(K.clip(y_true, 0, 1))\n",
    "        y_true = K.round(K.clip((y_true-1)*-1, 0, 1))\n",
    "\n",
    "        y_pred= K.round(K.clip(y_pred, 0, 1))\n",
    "        y_pred = K.round(K.clip((y_pred-1)*-1, 0, 1))\n",
    "\n",
    "        #count the number of correct Fake prediction\n",
    "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "\n",
    "        #count number of entries predicted as Fake\n",
    "        Pred_Fakes = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    \n",
    "        precision = TP / (Pred_Fakes+K.epsilon())\n",
    "        return precision \n",
    "    \n",
    "    #get precision and recall score of Fake class\n",
    "    precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom weighted loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_loss_function(labels, logits, weight=0.33):\n",
    "\n",
    "    pos_weight = tf.constant(weight)\n",
    "    return tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(labels=labels, logits=logits, pos_weight=pos_weight))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "\n",
    "    #load data \n",
    "    df_true = pd.read_csv(\"./dataset/Authentic-48K.csv\")\n",
    "    df_fake = pd.read_csv(\"./dataset/Fake-1K.csv\")\n",
    "    df = pd.concat([df_fake, df_true])\n",
    "    df = shuffle(df,random_state=50)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "    df = df[[\"articleID\", \"content\", \"label\"]]\n",
    "\n",
    "    #split data into 80:20 (train:test) ratio\n",
    "    train, test = train_test_split(df, test_size=0.20)\n",
    "    \n",
    "    #split the train set into further 90:10 (train:dev) ratio\n",
    "    train, dev = train_test_split(train, test_size=0.10)\n",
    "\n",
    "\n",
    "    #only 100 data are used to check the code\n",
    "    X_train = train['content'][:100].ravel().tolist()\n",
    "    Y_train = train['label'][:100]\n",
    "\n",
    "    X_dev = dev['content'][:100].ravel().tolist()\n",
    "    Y_dev = dev['label'][:100]\n",
    "\n",
    "    X_test = test['content'][:100].ravel().tolist()\n",
    "    Y_test = test['label'][:100]\n",
    "\n",
    "\n",
    "    #convert Y into one hot encoding\n",
    "    Y_train = tf.one_hot(Y_train,depth=2)\n",
    "    Y_dev = tf.one_hot(Y_dev,depth=2)\n",
    "    Y_test = tf.one_hot(Y_test,depth=2)\n",
    "    \n",
    "    return X_train, Y_train, X_dev, Y_dev, X_test, Y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classifier for train, test and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(X_train, X_dev, Y_train, Y_dev, X_test, Y_test, config, model_name):\n",
    "\n",
    "    \"\"\"Train and Save model for test and evaluation\"\"\"\n",
    "\n",
    "    #set random seed to make results reproducible  \n",
    "    np.random.seed(config['seed'])\n",
    "    tf.random.set_seed(config['seed'])\n",
    "    python_random.seed(config['seed'])\n",
    "\n",
    "    #set model parameters \n",
    "    max_length  =  config['max_length']\n",
    "    learning_rate =  config[\"learning_rate\"]\n",
    "    epochs = config[\"epochs\"]\n",
    "    patience = config[\"patience\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "\n",
    "    if config[\"loss\"].upper() == \"CUSTOM\":\n",
    "        loss_function = weighted_loss_function\n",
    "    elif config[\"loss\"].upper() == \"BINARY\":\n",
    "        loss_function = BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "    if config['optimizer'].upper() == \"ADAM\":\n",
    "        optim = Adam(learning_rate=learning_rate)\n",
    "    elif config['optimizer'].upper() == \"SGD\":\n",
    "        optim = SGD(learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "    lm = config[\"model\"]\n",
    "        \n",
    "    #set tokenizer according to pre-trained model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(lm)\n",
    "    \n",
    "    #get transformer text classification model based on pre-trained model\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(lm, num_labels=2)\n",
    "    \n",
    "    #transform raw texts into model input \n",
    "    tokens_train = tokenizer(X_train, padding=True, \n",
    "                             max_length=max_length,\n",
    "                             truncation=True, \n",
    "                             return_tensors=\"np\").data\n",
    "    tokens_dev = tokenizer(X_dev, \n",
    "                           padding=True, \n",
    "                           max_length=max_length,\n",
    "                           truncation=True, \n",
    "                           return_tensors=\"np\").data\n",
    "\n",
    "    tokens_test = tokenizer(X_test, \n",
    "                           padding=True, \n",
    "                           max_length=max_length,\n",
    "                           truncation=True, \n",
    "                           return_tensors=\"np\").data\n",
    "   \n",
    "\n",
    "    model.compile(loss=loss_function, optimizer=optim, metrics=['accuracy',f1_score])\n",
    "\n",
    "    #callbacks for ealry stopping and saving model history\n",
    "    es = EarlyStopping(monitor=\"val_f1_score\", patience=patience, restore_best_weights=True, mode='max')\n",
    "    history_logger = CSVLogger('log/'+model_name+\"-HISTORY.csv\", separator=\",\", append=True)\n",
    "\n",
    "    #train models\n",
    "    model.fit(tokens_train, \n",
    "              Y_train, \n",
    "              verbose=0, \n",
    "              epochs=epochs,\n",
    "              batch_size= batch_size, \n",
    "              validation_data=(tokens_dev, Y_dev), \n",
    "              callbacks=[es, history_logger, \n",
    "              TqdmCallback(verbose=2)])\n",
    "    \n",
    "    #save models in directory\n",
    "    model.save_pretrained(save_directory='Models/'+model_name)\n",
    "\n",
    "    #return prediction on test data\n",
    "    Y_pred = model.predict(tokens_test, batch_size=1)[\"logits\"]\n",
    "\n",
    "    Y_pred = np.argmax(Y_pred, axis=1)\n",
    "    Y_test = np.argmax(Y_test, axis=1)\n",
    "\n",
    "    return Y_test, Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train & evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_dev, Y_dev, X_test, Y_test = load_data()\n",
    "config = {\n",
    "\n",
    "\n",
    "    \"model\": \"sagorsarker/bangla-bert-base\",\n",
    "    \"max_length\" : 512,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"epochs\": 1,\n",
    "    \"patience\": 3,\n",
    "    \"batch_size\": 1,\n",
    "    \"loss\": \"binary\",\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"seed\": 1234\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "if not os.path.exists('log'):\n",
    "    os.mkdir('log')\n",
    "if not os.path.exists('Models'):\n",
    "    os.mkdir('Models')\n",
    "\n",
    "Y_test, Y_pred = classifier(X_train,X_dev, Y_train, Y_dev, X_test, Y_test, config, 'bangla-bert-base')\n",
    "print(classification_report(Y_test,Y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "37e5e45af5406b3675df4156b6f7beb8fd4560d92f598417c49f59a9ab3fd5f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
